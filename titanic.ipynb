# titanic_model.py
# Enkel ML-pipeline for Titanic-datasettet:
# - Leser data fra data/train.csv
# - Preprosessering og feature engineering
# - Trener Logistic Regression og Random Forest
# - Evaluerer og skriver ut resultater
# - Lagrer beste modell til models/random_forest.pkl

import os
import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib

DATA_PATH = "data/train.csv"
MODEL_DIR = "models"
BEST_MODEL_PATH = os.path.join(MODEL_DIR, "random_forest.pkl")

def load_data(path=DATA_PATH):
    if not os.path.exists(path):
        raise FileNotFoundError(
            f"Fant ikke {path}. Legg Kaggle-filen 'train.csv' i data/ først."
        )
    df = pd.read_csv(path)
    return df

def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:
    # Behold kun relevante kolonner + target
    cols = [
        "Survived", "Pclass", "Sex", "Age", "SibSp", "Parch",
        "Fare", "Embarked"
    ]
    df = df[cols].copy()

    # FamilySize (enkelt nytt feature)
    df["FamilySize"] = df["SibSp"] + df["Parch"] + 1

    return df

def build_pipelines(numeric_features, categorical_features):
    numeric_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="median"))
    ])

    categorical_transformer = Pipeline(steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features),
        ]
    )

    logreg = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", LogisticRegression(max_iter=1000))
    ])

    rf = Pipeline(steps=[
        ("preprocess", preprocessor),
        ("model", RandomForestClassifier(
            n_estimators=300,
            max_depth=None,
            random_state=42
        ))
    ])

    return logreg, rf

def main():
    # 1) Last data
    df = load_data()
    df = feature_engineering(df)

    # 2) Del i X/y
    y = df["Survived"].astype(int)
    X = df.drop(columns=["Survived"])

    # 3) Definer kolonner
    numeric_features = ["Age", "Fare", "SibSp", "Parch", "FamilySize", "Pclass"]
    categorical_features = ["Sex", "Embarked"]

    # 4) Bygg pipelines
    logreg, rf = build_pipelines(numeric_features, categorical_features)

    # 5) Train/test-splitt
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 6) Tren modeller
    logreg.fit(X_train, y_train)
    rf.fit(X_train, y_train)

    # 7) Evaluer
    preds_logreg = logreg.predict(X_test)
    preds_rf = rf.predict(X_test)

    acc_logreg = accuracy_score(y_test, preds_logreg)
    acc_rf = accuracy_score(y_test, preds_rf)

    print("=== Resultater ===")
    print(f"Logistic Regression accuracy: {acc_logreg:.3f}")
    print(f"Random Forest accuracy      : {acc_rf:.3f}")

    print("\nClassification report (Random Forest):")
    print(classification_report(y_test, preds_rf, digits=3))

    print("Confusion matrix (Random Forest):")
    print(confusion_matrix(y_test, preds_rf))

    # 8) Kryssvalidering (valgfritt, gir mer robusthets-signal)
    cv_scores_rf = cross_val_score(rf, X, y, cv=5, scoring="accuracy")
    print(f"\nRandom Forest CV accuracy (5-fold): {cv_scores_rf.mean():.3f} ± {cv_scores_rf.std():.3f}")

    # 9) Lagre beste modell
    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(rf, BEST_MODEL_PATH)
    print(f"\nLagret beste modell til: {BEST_MODEL_PATH}")

if __name__ == "__main__":
    main()
